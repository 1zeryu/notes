# 深度学习数据处理

## 训练_开发_测试数据

​	在如今的大数据时代，通常将数据分成用于训练模型用的训练集，用于调整超参数用的训练验证集，用于做出无偏差评估的测试集

## 偏差(bias)，方差(varience)

​	高偏差会导致欠拟合，高方差会导致过拟合

​	由于我们的最优目标是达到人类的学习水平，所以我们可以把人的误差设定为基础误差，

![1644651482828](C:\Users\坤\AppData\Roaming\Typora\typora-user-images\1644651482828.png)

​	正则化可以降低方差

解决方案：

* 高偏差： 构建更大的神经网络 ，增加新的特征
* 高方差： 使用更大的数据集，正则化

### L2正则化

​	我们做正则化的前提是定义代价函数，目前的正则话的方法是：

![1644653612837](C:\Users\坤\AppData\Roaming\Typora\typora-user-images\1644653612837.png)

神经网络的正则化：
$$
J(w^{[1]},b^{[1]},{\dots},w^{[l]},b^{[l]}) = \frac 1 m \sum_{i=1}^m{\delta}(y^i,y^{[i]}) \\
Frobenius:||w^{[l]}|| = \sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} \\
\mathrm{d}w = (back pop) + \frac {\lambda} m w^{[l]} \\
w^{[l]} = w^{[l]} - {\alpha} \mathrm{d}w
$$

## Dropout正则化

​	![1644656035884](C:\Users\坤\AppData\Roaming\Typora\typora-user-images\1644656035884.png)

implement：

1. inverted dropout(反向随机失活 )

其他正则化方法：

* data argumentation
* early stopping

## 归一化输出

​	通过以下几步，将数据转化成合适的范围，且更具学习效率
$$
u = \frac 1 m \sum_{i=1}^mx^{(i)} \\
x = x - u\\
{\sigma}^2 = \frac 1 m \sum_{i=1}^m x^2
\\x = x/ u
$$

## 梯度消失和梯度爆炸 

## Grading checking